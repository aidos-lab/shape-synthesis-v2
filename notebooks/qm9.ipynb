{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preperation",
   "id": "e09b25bb29e6af6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:27:55.474842Z",
     "start_time": "2025-07-30T10:27:55.415432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from shape_synthesis.datasets.transforms import get_transform \n",
    "from shape_synthesis.datasets.qm9 import DataConfig, create_dataset, get_dataloaders\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# When turning dev=True, the dataset will be created with a smaller number of points for faster testing\n",
    "dev = False\n",
    "\n",
    "res = 32\n",
    "num_dir = 16\n",
    "x_dim  = res * num_dir \n",
    "\n",
    "num_examples = -1\n",
    "\n",
    "# VAE settings\n",
    "hidden_dim = 400\n",
    "latent_dim = 100\n",
    "lr_vae = 1e-3\n",
    "epochs_vae = 150\n",
    "\n",
    "# MLP settings\n",
    "hidden_dims_mlp = [64, 128, 256]\n",
    "lr_mlp = 1e-3\n",
    "epochs_mlp = 100\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "config = DataConfig(\n",
    "        root=\"./data\",\n",
    "        raw=\"./data/raw\",\n",
    "        batch_size=batch_size,\n",
    "    )"
   ],
   "id": "bed8735f52f1068b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_dataloaders' from 'shape_synthesis.datasets.qm9' (/Users/jschmidt/Dev/shape-synthesis-v2/shape_synthesis/datasets/qm9.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataLoader, TensorDataset\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mshape_synthesis\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtransforms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_transform \n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mshape_synthesis\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mqm9\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataConfig, create_dataset, get_dataloaders\n\u001B[1;32m     15\u001B[0m DEVICE \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# When turning dev=True, the dataset will be created with a smaller number of points for faster testing\u001B[39;00m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'get_dataloaders' from 'shape_synthesis.datasets.qm9' (/Users/jschmidt/Dev/shape-synthesis-v2/shape_synthesis/datasets/qm9.py)"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:27:54.401309Z",
     "start_time": "2025-07-30T10:27:52.180235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_type = \"dev\" if dev else \"prod\"\n",
    "path = f\"{config.root}/qm9/{dataset_type}\"\n",
    "\n",
    "create_dataset(config, dev=dev)\n",
    "\n",
    "train_data = torch.load(f\"{path}/train.pt\")\n",
    "\n",
    "x = train_data\n",
    "\n",
    "print(72 * \"=\")\n",
    "print(\"Info\")\n",
    "print(\"Min\", x.min(dim=0)[0])\n",
    "print(\"Max\", x.max(dim=0)[0])\n",
    "print(\"Norm\", torch.norm(x, dim=1).max())\n",
    "print(72 * \"=\")"
   ],
   "id": "4955c3cc5ea9aa12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod\n",
      "Creating: ./data/qm9\n",
      "Creating: ./data/raw\n",
      "Saving config to ./data/qm9/prod/config.yaml\n",
      "========================================================================\n",
      "Info\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QM9' object has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;241m72\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInfo\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMin\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmin\u001B[49m(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMax\u001B[39m\u001B[38;5;124m\"\u001B[39m, x\u001B[38;5;241m.\u001B[39mmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNorm\u001B[39m\u001B[38;5;124m\"\u001B[39m, torch\u001B[38;5;241m.\u001B[39mnorm(x, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mmax())\n",
      "File \u001B[0;32m~/micromamba/envs/shape-synthesis/lib/python3.10/site-packages/torch_geometric/data/in_memory_dataset.py:318\u001B[0m, in \u001B[0;36mInMemoryDataset.__getattr__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    315\u001B[0m         data_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices()]\n\u001B[1;32m    316\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m Batch\u001B[38;5;241m.\u001B[39mfrom_data_list(data_list)[key]\n\u001B[0;32m--> 318\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    319\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'QM9' object has no attribute 'min'"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:24:51.271728Z",
     "start_time": "2025-07-30T10:24:51.082790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "colors = { 1:'red', 6: 'blue', 7: 'black', 8: 'green'}\n",
    "atom_types = {1: 'H', 6: 'C', 7: 'N', 8: 'O'}\n",
    "\n",
    "pos = train_data[0].pos\n",
    "z = train_data[0].z\n",
    "\n",
    "mask_h = z == 1\n",
    "mask_c = z == 6\n",
    "mask_n = z == 7\n",
    "mask_o = z == 8\n",
    "mask_f = z == 9\n",
    "\n",
    "plt.scatter(pos[mask_c][:, 0], pos[mask_c][:, 1], color='black', label='C')\n",
    "plt.scatter(pos[mask_n][:, 0], pos[mask_n][:, 1], color='green', label='N')\n",
    "plt.scatter(pos[mask_o][:, 0], pos[mask_o][:, 1], color='red', label='O')\n",
    "plt.scatter(pos[mask_h][:, 0], pos[mask_h][:, 1], color='grey', label='H')\n",
    "plt.scatter(pos[mask_f][:, 0], pos[mask_f][:, 1], color='blue', label='F')\n"
   ],
   "id": "d19b5da2ebb4340f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3]) torch.Size([7, 3]) torch.Size([2, 3]) torch.Size([0, 3]) torch.Size([0, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x33c549750>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T10:31:54.047120Z",
     "start_time": "2025-07-30T10:31:53.821013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preparing the channeled ECT out of the QM9 dataset\n",
    "\n",
    "transform = get_transform(device=DEVICE, resolution=res, num_thetas=num_dir, ambient_dimension=3)\n",
    "\n",
    "train_ects = []\n",
    "train_number_atoms = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    pos = train_data[i].pos\n",
    "    z = train_data[i].z\n",
    "    \n",
    "    # define filtration via atom type and channel\n",
    "    mask_c = z == 6\n",
    "    mask_n = z == 7 + mask_c\n",
    "    mask_o = z == 8 + mask_n\n",
    "    mask_f = z == 9 + mask_o\n",
    "    mask_h = z == 1 + mask_f\n",
    "    \n",
    "    \n",
    "    ect_c = torch.flatten(transform(pos[mask_c]).movedim(-1,-1), start_dim=1)\n",
    "    ect_n = torch.flatten(transform(pos[mask_n]).movedim(-1,-1), start_dim=1)\n",
    "    ect_o = torch.flatten(transform(pos[mask_o]).movedim(-1,-1), start_dim=1)\n",
    "    ect_f = torch.flatten(transform(pos[mask_f]).movedim(-1,-1), start_dim=1)\n",
    "    ect_h = torch.flatten(transform(pos[mask_h]).movedim(-1,-1), start_dim=1)\n",
    "    \n",
    "    ect = torch.stack([ect_c, ect_n, ect_o, ect_f, ect_h], dim=0)\n",
    "    train_ects.append(ect)\n",
    "    train_number_atoms.append(len(pos))\n",
    "    \n",
    "\n",
    "torch_train_ects = torch.cat(train_ects, dim=0)\n",
    "torch_train_number_atoms = torch.cat(train_number_atoms, dim=0)\n",
    "train_dataset = TensorDataset(torch_train_ects, torch_train_number_atoms)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "dc2e1a3906f658a3",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (7) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[48], line 20\u001B[0m\n\u001B[1;32m     16\u001B[0m mask_f \u001B[38;5;241m=\u001B[39m z \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m9\u001B[39m \u001B[38;5;241m+\u001B[39m mask_o\n\u001B[1;32m     17\u001B[0m mask_h \u001B[38;5;241m=\u001B[39m z \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m mask_f\n\u001B[0;32m---> 20\u001B[0m ect_c \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpos\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmask_c\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mmovedim(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     21\u001B[0m ect_n \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(transform(pos[mask_n])\u001B[38;5;241m.\u001B[39mmovedim(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     22\u001B[0m ect_o \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(transform(pos[mask_o])\u001B[38;5;241m.\u001B[39mmovedim(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Dev/shape-synthesis-v2/shape_synthesis/datasets/transforms.py:80\u001B[0m, in \u001B[0;36mEctTransform.__call__\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mect_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Dev/shape-synthesis-v2/dect/dect/ect.py:180\u001B[0m, in \u001B[0;36mcompute_ect_point_cloud\u001B[0;34m(x, v, radius, resolution, scale, normalize)\u001B[0m\n\u001B[1;32m    176\u001B[0m lin \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlinspace(\n\u001B[1;32m    177\u001B[0m     start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mradius, end\u001B[38;5;241m=\u001B[39mradius, steps\u001B[38;5;241m=\u001B[39mresolution, device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m    178\u001B[0m )\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    179\u001B[0m nh \u001B[38;5;241m=\u001B[39m (x \u001B[38;5;241m@\u001B[39m v)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 180\u001B[0m ecc \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39msigmoid(scale \u001B[38;5;241m*\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnh\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    181\u001B[0m ect \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(ecc, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalize:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (32) must match the size of tensor b (7) at non-singleton dimension 0"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decoder MLP",
   "id": "d20602fbcf98d1d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class PointCloudMLP(nn.Module):\n",
    "    def __init__(self, x_input: int, hidden_dims: list, max_no_points: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_input (int): Dimension of input vector.\n",
    "            hidden_dims (list): List of hidden layer sizes, e.g. [128, 256].\n",
    "            no_points (int): Number of output points (point cloud size).\n",
    "        \"\"\"\n",
    "        super(PointCloudMLP, self).__init__()\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        layers = []\n",
    "        input_dim = x_input\n",
    "        \n",
    "        for h in hidden_dims:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "\n",
    "        # Final layer: outputting (no_points * 2) values\n",
    "        layers.append(nn.Linear(input_dim, max_no_points * 3))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.no_points = max_no_points\n",
    "\n",
    "    def forward(self, x, no_atoms: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, x_input).\n",
    "            no_atoms (int): Number of atoms in molecule.\n",
    "        Returns:\n",
    "            torch.Tensor: Output point cloud of shape (batch_size, no_points, 2).\n",
    "        \"\"\"\n",
    "        out = self.network(x)  # (batch_size, no_points * 2)\n",
    "        out = out.view(-1, self.no_points, 3)[:, no_atoms]  # Reshape to (batch_size, no_points, 2)\n",
    "        return out"
   ],
   "id": "5188fec7ed9ecae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Add a loss function to allow permutation invariance\n",
    "\n",
    "def chamfer_distance(p1, p2):\n",
    "    \"\"\"\n",
    "    p1, p2: (batch, n, 2)\n",
    "    \"\"\"\n",
    "    diff_1 = torch.cdist(p1, p2)  # (batch, n, n)\n",
    "    min_dist_1 = diff_1.min(dim=2)[0]  # (batch, n)\n",
    "    min_dist_2 = diff_1.min(dim=1)[0]  # (batch, n)\n",
    "    return (min_dist_1.mean() + min_dist_2.mean())"
   ],
   "id": "da2f01a6257902e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_input = res*num_dir\n",
    "model2 = PointCloudMLP(x_input, hidden_dims_mlp, num_pts)\n",
    "\n",
    "optimizer = Adam(model2.parameters(), lr=lr_mlp)\n",
    "loss_fn = chamfer_distance\n",
    "\n",
    "for epoch in range(epochs_mlp):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.view(batch_size, x_input)\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_hat = model2(x)\n",
    "        loss = loss_fn(y, y_hat)\n",
    "        \n",
    "        overall_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / ((batch_idx+1)*batch_size))"
   ],
   "id": "44ceffa28bbe6e9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
